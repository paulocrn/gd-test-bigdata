{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "faa31b16-16ce-49cd-9e71-a53a9fd76c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerias Importadas\n"
     ]
    }
   ],
   "source": [
    "# Importe Librerias\n",
    "import calendar\n",
    "import sys\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Librerias Importadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c09df120-0723-474e-a16d-f0f48f5e6e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/gd-test-bigdata/notebooks/EJERCICIO-1/postgresql-42.7.5.jar\n",
      "Función creada\n"
     ]
    }
   ],
   "source": [
    "# Conexión a bd\n",
    "sys.path.append(os.path.abspath(os.getcwd() + \"/..\"))\n",
    "\n",
    "from db_conexion import get_spark_session, write_table, get_postgres_connection\n",
    "\n",
    "spark = get_spark_session()\n",
    "\n",
    "print(spark.conf.get(\"spark.driver.extraClassPath\"))\n",
    "\n",
    "def execute_postgres_query(query):\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\",\n",
    "        host=\"localhost\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        print(\"Sentencia ejecutada correctamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al ejecutar la consulta: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "print(\"Función creada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "130c3adb-5488-4092-b3aa-ac4f54e31b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función verificar si tabla existe\n",
    "def table_exists(spark, table_name):\n",
    "    url, properties = get_postgres_connection()\n",
    "    \n",
    "    df = spark.read.jdbc(url=url, table=\"information_schema.tables\", properties=properties)\n",
    "\n",
    "    exists = df.filter(df[\"table_name\"] == table_name).count() > 0\n",
    "\n",
    "    return exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53b4c879-3e69-4609-8acf-5c837058f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia ejecutada correctamente.\n",
      "Tabla 'clientes' creada correctamente.\n",
      "Sentencia ejecutada correctamente.\n",
      "Tabla 'productos' creada correctamente.\n",
      "Sentencia ejecutada correctamente.\n",
      "Tabla 'corresponsales' creada correctamente.\n",
      "Sentencia ejecutada correctamente.\n",
      "Tabla 'ordenes' creada correctamente.\n",
      "Sentencia ejecutada correctamente.\n",
      "Tabla 'detalles_ordenes' creada correctamente.\n",
      "Proceso de creación de tablas completado.\n"
     ]
    }
   ],
   "source": [
    "# Creación de tablas con claves foráneas\n",
    "ddl_statements = {\n",
    "    \"clientes\": \"\"\"CREATE TABLE clientes (\n",
    "        id BIGSERIAL PRIMARY KEY,\n",
    "        nombre VARCHAR(100) NOT NULL,\n",
    "        email VARCHAR(100) UNIQUE NOT NULL,\n",
    "        telefono VARCHAR(20)\n",
    "    );\"\"\",\n",
    "    \"productos\": \"\"\"CREATE TABLE productos (\n",
    "        id BIGSERIAL PRIMARY KEY,\n",
    "        nombre VARCHAR(100) NOT NULL,\n",
    "        descripcion TEXT,\n",
    "        precio NUMERIC(10,5) NOT NULL\n",
    "    );\"\"\",\n",
    "    \"corresponsales\": \"\"\"CREATE TABLE corresponsales (\n",
    "        id BIGSERIAL PRIMARY KEY,\n",
    "        nombre VARCHAR(255) NOT NULL,\n",
    "        direccion VARCHAR(255),\n",
    "        telefono VARCHAR(20),\n",
    "        email VARCHAR(255),\n",
    "        fecha DATE DEFAULT CURRENT_DATE\n",
    "    );\"\"\",\n",
    "    \"ordenes\": \"\"\"CREATE TABLE ordenes (\n",
    "        id BIGSERIAL,\n",
    "        fecha DATE DEFAULT CURRENT_DATE,\n",
    "        id_cliente BIGINT REFERENCES clientes(id) ON DELETE CASCADE,\n",
    "        id_corresponsal BIGINT REFERENCES corresponsales(id) ON DELETE SET NULL,\n",
    "        PRIMARY KEY (id, fecha)\n",
    "    ) PARTITION BY RANGE (fecha);\n",
    "    \"\"\",\n",
    "    \"detalles_ordenes\": \"\"\"CREATE TABLE detalles_ordenes (\n",
    "        id BIGSERIAL PRIMARY KEY,\n",
    "        id_orden BIGINT,\n",
    "        fecha DATE,\n",
    "        id_producto BIGINT REFERENCES productos(id) ON DELETE CASCADE,\n",
    "        cantidad INT NOT NULL,\n",
    "        precio NUMERIC(10,5) NOT NULL,\n",
    "        FOREIGN KEY (id_orden, fecha) REFERENCES ordenes(id, fecha) ON DELETE CASCADE\n",
    "    );\"\"\"\n",
    "}\n",
    "\n",
    "# Ejecutar los comandos SQL solo si la tabla no existe\n",
    "for table, ddl in ddl_statements.items():\n",
    "    if not table_exists(spark, table):\n",
    "        execute_postgres_query(ddl)\n",
    "\n",
    "        print(f\"Tabla '{table}' creada correctamente.\")\n",
    "    else:\n",
    "        print(f\"Tabla '{table}' ya existe, saltando creación.\")\n",
    "\n",
    "\n",
    "print(\"Proceso de creación de tablas completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ff701-caa9-4872-9e3b-927adb035834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
